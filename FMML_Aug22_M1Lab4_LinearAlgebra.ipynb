{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopathotisrinadh21/FMML_LAB_ASSESSMENTS/blob/main/FMML_Aug22_M1Lab4_LinearAlgebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU9LZjQxiQT2"
      },
      "source": [
        "# Transforming data using linear algebra\n",
        "\n",
        "FMML Module 1, Lab 4<br>\n",
        "Module Coordinator: Amit Pandey [amit.pandey@research.iiit.ac.in]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBe4ZA32UTbv"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mqG-dafVpya"
      },
      "source": [
        "#@title\n",
        "\n",
        "from IPython.display import Latex as lt\n",
        "#Plotting functions \n",
        "#(You DON'T need to understand how these functions)\n",
        "\n",
        "# function to plot a grid \n",
        "def plotGrid(transform, unit, linestyle = ':', fig=None, ax=None):\n",
        "  lim1 = -100\n",
        "  lim2 = 100\n",
        "  def mat2xy(start, end):\n",
        "    if len(start.shape)==1:\n",
        "      start = np.expand_dims(start,0)\n",
        "      #print(start)\n",
        "      end = np.expand_dims(end,0)\n",
        "      #print(end)\n",
        "    #print(\"\\n\",len(start))\n",
        "    #print(\"\\n start is:\", start)\n",
        "    nan = np.ones(len(start))*np.nan\n",
        "    #print(\"\\n\",nan)\n",
        "\n",
        "    x = np.stack((start[:,0], end[:,0], nan)).T.reshape(-1)\n",
        "    #print(\"\\n x is:\",x)\n",
        "    y = np.stack((start[:,1], end[:,1], nan)).T.reshape(-1)\n",
        "    #print(\"\\n y is:\",y)\n",
        "    return x, y\n",
        "\n",
        "  def parallellines(axis, addend, lines, unit):\n",
        "    #print(\"addend:\",addend)\n",
        "    addend = np.repeat(np.expand_dims(addend,0), lines*2, 0) \n",
        "    #print(\"\\nnew addend:\", addend)\n",
        "    unit = np.expand_dims(np.arange(-lines, lines)*unit,1)\n",
        "    unit = unit-lines\n",
        "    addend = addend*unit\n",
        "    lines = np.expand_dims(axis,0) + addend\n",
        "    return np.concatenate((lines, lines*-1))\n",
        "\n",
        "  if fig is None:\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "  transform = transform.astype(np.float)\n",
        "  xaxis = transform[0]\n",
        "  #print(\"\\n x axis\", xaxis)\n",
        "  yaxis = transform[1]\n",
        "  #print(\"\\n y axis\", yaxis)\n",
        "    \n",
        "  # plot lines parallel to the x axis\n",
        "  lines1= parallellines(xaxis*lim1, yaxis, 100,unit )\n",
        "  lines2 = parallellines(xaxis*lim2, yaxis, 100,unit )\n",
        "  x,y = mat2xy(lines1, lines2)\n",
        "  plt.plot(x,y, linestyle+'k', linewidth=0.5)\n",
        "  # plot x axis\n",
        "  x,y = mat2xy(xaxis*lim1, xaxis*lim2)\n",
        "  plt.plot(x,y,linestyle, color = '#440077')\n",
        "\n",
        "  # plot  lines parallel to the y axis\n",
        "  lines1= parallellines(yaxis*lim1, xaxis, 100,unit)\n",
        "  lines2 = parallellines(yaxis*lim2, xaxis, 100,unit)\n",
        "  x,y = mat2xy(lines1, lines2)\n",
        "  plt.plot(x,y, linestyle+'k', linewidth=0.5)\n",
        "  # plot y axis\n",
        "  x,y = mat2xy(yaxis*lim1, yaxis*lim2)\n",
        "  plt.plot(x,y,linestyle, color= '#aa5500')\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "def plotData(X, y, xlabel = 'hole', ylabel = 'bound', fig=None, ax = None):\n",
        "\n",
        "  if fig is None:\n",
        "    fig, ax = plt.subplots()\n",
        "  for ii in range(nclasses):\n",
        "    plt.scatter(X[y==ii,0], X[y==ii, 1])\n",
        "  plt.legend([str(i) for i in range(nclasses)])\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  lim2 = X.max() \n",
        "  lim1 = X.min() \n",
        "  add = abs(lim1-lim2)/5\n",
        "  return fig, ax\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5dmEXxaktp"
      },
      "source": [
        "# Matrix transformations on data\n",
        "\n",
        "A 2D coordinate system is defined by its basis vectors, i and j. In the standard coordinate system (Let us call it T0), the basis vectors are\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ 0 \\end{aligned} \\right\\} \n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 1\\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "We can use any two vectors as basis vectors for a new coordinate system as long as they are not colinear. To be a basis all that is required is linear independence and they must span the space. For example, let us call this new coordinate system T1:\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ -1 \\end{aligned} \\right\\} \n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 2 \\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "Suppose we have a point [a,b] in the T1 coordinate system. Its representation in the standard system T0 can be obtained by the following matrix multiplication:\n",
        "\n",
        "$$ \\begin{equation} \n",
        "\\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} = \n",
        "\\left\\{  \\begin{aligned}&1 & 0 \\\\ -&1 & 2 \\end{aligned} \\right\\}\n",
        "\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "where the columns of the matrix are the basis vectors of T1. \n",
        "\n",
        "Let us see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8SCFKqfbM-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08953fc4-6eb8-4d3c-f164-f7ec8b4faec8"
      },
      "source": [
        "T0 = np.array([[1,0],[0,1]])\n",
        "T1 = np.array([[1,0], [-1,2]])\n",
        "\n",
        "data1 = np.array([5,4]) # the data in T1 coordinate system\n",
        "data0 = np.matmul(T1, data1) # the data in T0 coordinate system\n",
        "\n",
        "print('Data in T0 = ', data0)\n",
        "print('Data in T1 = ', data1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data in T0 =  [5 3]\n",
            "Data in T1 =  [5 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIfEWZ1RQeve"
      },
      "source": [
        "We can visualize this below. T0 is shown with dotted lines and T1 is shown with solid lines. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IMPORTANT : IF T IS A NON STANDARD BASIS / COORDINATE SYSTEM, AND P IS VECTOR/COORDINATE OF A POINT IN THIS T SYSTEM, THEN COORDINATE OF P IN STANDARD BASIS\n",
        "## SYSTEM IS : PSTANDARD = T*P [* DENOTES MATRIX MULTIPLICATION]\n",
        "\n",
        "\n",
        "## AND IF Pnonstandard = T^-1 * Pstandard\n",
        "\n",
        "https://towardsdatascience.com/essential-math-for-data-science-basis-and-change-of-basis-f7af2348d463\n",
        "\n",
        "https://math.stackexchange.com/questions/165563/change-to-standard-basis"
      ],
      "metadata": {
        "id": "5DAU_OWLczxp",
        "outputId": "121882e5-9d3a-4206-acd8-259bf1e2dda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a59452d3bb20>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    https://towardsdatascience.com/essential-math-for-data-science-basis-and-change-of-basis-f7af2348d463\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taCUcAMEpBlt"
      },
      "source": [
        "## EACH COLUMN OF T == BASIS MATRIX IS THE iTH AND jTH VECTOR (IMAGINE AS X AND Y AXIS EQUIVALENT OF NON STANDARD SYSTEM)\n",
        "\n",
        "## FOR STANDARD SYSTEM ==> iTH VECTOR == X AXIS == [1 0] AND jTH VECTOR == Y AXIS == [0 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0A2Bc36qHg0"
      },
      "source": [
        "fig, ax = plotGrid(T1.T, 1,'-') # custom plotting function, no need to understand this\n",
        "#plotGrid(T0.T, 1, fig=fig, ax=ax) # custom plotting function, no need to understand this\n",
        "plt.scatter(data0[0], data0[1])\n",
        "ax.set_xlim(-10,10)\n",
        "ax.set_ylim(-10,10)\n",
        "ax.set_xticks([]);\n",
        "ax.set_yticks([]);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSuTaeSDQoYK"
      },
      "source": [
        "fig, ax = plotGrid(T1.T, 1,'-') # custom plotting function, no need to understand this\n",
        "plotGrid(T0.T, 1, fig=fig, ax=ax) # custom plotting function, no need to understand this\n",
        "plt.scatter(data0[0], data0[1])\n",
        "ax.set_xlim(-10,10)\n",
        "ax.set_ylim(-10,10)\n",
        "ax.set_xticks([]);\n",
        "ax.set_yticks([]);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ927lleopa5"
      },
      "source": [
        "\n",
        "fig, ax = plotGrid(T0.T, 1,'-') # custom plotting function, no need to understand this\n",
        "plotGrid(T0.T, 1, fig=fig, ax=ax) # custom plotting function, no need to understand this\n",
        "plt.scatter(data0[0], data0[1])\n",
        "ax.set_xlim(-10,10)\n",
        "ax.set_ylim(-10,10)\n",
        "ax.set_xticks([]);\n",
        "ax.set_yticks([]);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUb3oYWIgdya"
      },
      "source": [
        "Look at the coordinates of the blue dot. In T0 (dotted lines), the position is [5,3] where it is [5,4] in T1. Feel free to experiment with different data points and coordinate systems. \n",
        "\n",
        "Remember that we can achieve the same thing by post-multiplying the transpose of the transformation matrix to the data. This will come in handy when transforming multiple data points at once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD4pqOMndf-4"
      },
      "source": [
        "data0_a = np.matmul(T1, data1)\n",
        "data0_b = np.matmul(data1, T1.T)\n",
        "print(data0_a)\n",
        "print(data0_b)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Kui3dSESPm"
      },
      "source": [
        "Why is transforming data useful? Data transformations cause the distance between data points to change. This will affect distance-based algorithms such as nearest neighbour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE0NbpYIC9ou"
      },
      "source": [
        "# let us define 3 points in T1\n",
        "A1 = np.array([3,3])\n",
        "B1 = np.array([2,-5])\n",
        "C1 = np.array([1,-1])\n",
        "\n",
        "# the corresponding points in T0:\n",
        "A0 = np.matmul(T1, A1)\n",
        "B0 = np.matmul(T1, B1)\n",
        "C0 = np.matmul(T1, C1)\n",
        "\n",
        "# function to calculate Euclidean distance:\n",
        "def dist(a, b):\n",
        "  diff = a-b\n",
        "  sq = diff*diff\n",
        "  return np.sqrt(sq.sum())\n",
        "\n",
        "# distance between the points in T1\n",
        "print('Distance between A and B in T1 = ', dist(A1, B1))\n",
        "print('Distance between B and C in T1 = ', dist(B1, C1))\n",
        "print('Distance between A and C in T1 = ', dist(A1, C1))\n",
        "\n",
        "print('')\n",
        "# distnace between the points in T0\n",
        "print('Distance between A and B in T0 = ', dist(A0, B0))\n",
        "print('Distance between B and C in T0 = ', dist(B0, C0))\n",
        "print('Distance between A and C in T0 = ', dist(A0, C0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE95JMniUtDm"
      },
      "source": [
        "We see that in T1, B and C are the closest whereas in T0, A and C are the closest. These kinds of changes will affect the predictions returned by the nearest neighbour algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFbpTSBdV29C"
      },
      "source": [
        "# Transformations on MNIST\n",
        "\n",
        "Let us experiment with a subset of the MNIST dataset. We will extract two features from the database for our experiment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMvUzC2GUawS"
      },
      "source": [
        "Functions for nearest neighbour, accuracy and feature extraction. (from previous labs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ayl0sViF5-"
      },
      "source": [
        "#@title\n",
        "\n",
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here \n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data \n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)\n",
        "\n",
        "def cumArray(img):\n",
        "  img2 = img.copy()\n",
        "  for ii in range(1, img2.shape[1]):\n",
        "    img2[ii,:] = img2[ii,:] + img2[ii-1,:]  # for every row, add up all the rows above it.\n",
        "  img2 = img2>0\n",
        "  return img2\n",
        "\n",
        "def getHolePixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  hole = hull & ~ (img>0) # remove the original digit to leave behind the holes\n",
        "  return hole\n",
        "\n",
        "def getHullPixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  return hull\n",
        "\n",
        "def minus(a, b):\n",
        "  return a & ~ b\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "  img = img.copy()>0  # binarize the image\n",
        "  rshift = np.roll(img, 1, 1)\n",
        "  lshift = np.roll(img, -1 ,1)\n",
        "  ushift = np.roll(img, -1, 0)\n",
        "  dshift = np.roll(img, 1, 0)\n",
        "  boundary = minus(img, rshift) | minus(img, lshift) | minus(img, ushift) | minus(img, dshift)\n",
        "  return boundary\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzF-9mGxUkWC"
      },
      "source": [
        "Get the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLioX9fTr6wF"
      },
      "source": [
        "def minus(a, b):\n",
        "  return a & ~ b\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "  img = img.copy()>0  # binarize the image\n",
        "  rshift = np.roll(img, 1, 1)\n",
        "  lshift = np.roll(img, -1 ,1)\n",
        "  ushift = np.roll(img, -1, 0)\n",
        "  dshift = np.roll(img, 1, 0)\n",
        "  boundary = minus(img, rshift) | minus(img, lshift) | minus(img, ushift) | minus(img, dshift)\n",
        "  return boundary\n",
        "\n",
        "def cumArray(img):\n",
        "  img2 = img.copy()\n",
        "  for ii in range(1, img2.shape[1]):\n",
        "    img2[ii,:] = img2[ii,:] + img2[ii-1,:]  # for every row, add up all the rows above it.\n",
        "  #print(img2)\n",
        "  img2 = img2>0\n",
        "  #print(img2)\n",
        "  return img2\n",
        "\n",
        "def getHolePixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  hole = hull & ~ (img>0) # remove the original digit to leave behind the holes\n",
        "  return hole\n",
        "\n",
        "\n",
        "def getHullPixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  return hull\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHz5BVmLUjzb"
      },
      "source": [
        "#loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "nclasses = 4\n",
        "\n",
        "# get only for the first 4 classes\n",
        "train_X = train_X[train_y<nclasses]\n",
        "train_y = train_y[train_y<nclasses]\n",
        "test_X = test_X[test_y<nclasses]\n",
        "test_y = test_y[test_y<nclasses]\n",
        "\n",
        "train_X = train_X[::100].copy() # We are only taking a subset of the training set\n",
        "train_y = train_y[::100].copy() # do the same to the labels\n",
        "test_X = test_X[::100].copy() # taking a subset of the test set. This code takes every 500th sample\n",
        "test_y = test_y[::100].copy()\n",
        "\n",
        "# get all the features\n",
        "train_hole = np.array([getHolePixels(i).sum() for i in train_X])\n",
        "test_hole = np.array([getHolePixels(i).sum() for i in test_X])\n",
        "train_bound = np.array([getBoundaryPixels(i).sum() for i in train_X])\n",
        "test_bound = np.array([getBoundaryPixels(i).sum() for i in test_X])\n",
        "# train_hull = np.array([getHullPixels(i).sum() for i in train_X])\n",
        "# test_hull = np.array([getHullPixels(i).sum() for i in test_X])\n",
        "# train_sum = np.sum(train_X, (1,2))/(28*28)\n",
        "# test_sum = np.sum(test_X, (1,2))/(28*28)\n",
        "\n",
        "# create the train and test set by combining the appropriate features\n",
        "train_feats = np.vstack((train_hole,train_bound)).transpose()\n",
        "test_feats = np.vstack((test_hole, test_bound)).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qGVrlnQCUy"
      },
      "source": [
        "Let us plot the samples and see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnqAj7lprnZs"
      },
      "source": [
        "train_feats.shape #248 samples and 2 is the number of features == holes and bound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RPqsTMFDNG8"
      },
      "source": [
        "train_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19ZAP2jvDZ19"
      },
      "source": [
        "train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saRzHfi9QAGd"
      },
      "source": [
        "# fix limits of x and y axis so that we can see what is going on\n",
        "xlim=[-100,300]\n",
        "ylim=[-100,300]\n",
        "fig, ax = plotData(train_feats, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf6ZCx7ctB_k"
      },
      "source": [
        "## Elongated in the x direction, as range of values features can take is different, we generally dont like this in ml models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCb4DikQP1ck"
      },
      "source": [
        "Check the baseline accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEs6cXR_tbRg"
      },
      "source": [
        "\n",
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here \n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data \n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxVr6bd9PzlI"
      },
      "source": [
        "test_pred = NN(train_feats, train_y, test_feats)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print('Baseline accuracy = ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl8Noo8pZRek"
      },
      "source": [
        "Let us try transforming the features and checking their accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dUPWRsEZKwo"
      },
      "source": [
        "transform = np.array([[0.5,-0.5],[0,2.5]])\n",
        "\n",
        "train_feats_t = np.matmul(train_feats, transform)\n",
        "test_feats_t = np.matmul(test_feats, transform)  # whatever transform we are applying to the training set should be applied to the test set also"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRH4VckHZaWv"
      },
      "source": [
        "fig, ax = plotData(train_feats_t, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j76AQrYFPm9"
      },
      "source": [
        "transform = np.array([[0.5,-0.5],[0,4.5]])\n",
        "\n",
        "train_feats_t = np.matmul(train_feats, transform)\n",
        "test_feats_t = np.matmul(test_feats, transform)  # whatever transform we are applying to the training set should be applied to the test set also\n",
        "\n",
        "fig, ax = plotData(train_feats_t, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DD4K9KitqSG"
      },
      "source": [
        "\n",
        "## observe how the plot has changed. Since the values of the transformation matrix were chosen in such a way that it brings about this elongation in y direction - ie along bound.\n",
        "## you can play with these values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9fknczfZdYF"
      },
      "source": [
        "test_pred = NN(train_feats_t, train_y, test_feats_t)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print('Accuracy after transform = ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFBQlAnNZ3on"
      },
      "source": [
        "## Questions:\n",
        "1. Experiment with different transformation matrices and check the accuracy\n",
        "2. Will the same transform used for these two features also work for other features?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANSWER FOR QUESTION 1**"
      ],
      "metadata": {
        "id": "syggEclBaZPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* GETTING THE DATA SET \n",
        "\n"
      ],
      "metadata": {
        "id": "Rwmqr891agS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "#loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "nclasses = 4\n",
        "\n",
        "# get only for the first 4 classes\n",
        "train_X = train_X[train_y<nclasses]\n",
        "train_y = train_y[train_y<nclasses]\n",
        "test_X = test_X[test_y<nclasses]\n",
        "test_y = test_y[test_y<nclasses]\n",
        "\n",
        "train_X = train_X[::100].copy() # We are only taking a subset of the training set\n",
        "train_y = train_y[::100].copy() # do the same to the labels\n",
        "test_X = test_X[::100].copy() # taking a subset of the test set. This code takes every 500th sample\n",
        "test_y = test_y[::100].copy()"
      ],
      "metadata": {
        "id": "xBgd3drzafM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* FUNCTION TO TRANSFORM THE DATA\n",
        "\n"
      ],
      "metadata": {
        "id": "jQeK17tiapfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tranformDataSet(transform, train_feats, test_feats):\n",
        "  train_feats_t = np.matmul(train_feats, transform)\n",
        "  test_feats_t = np.matmul(test_feats, transform)\n",
        "  return train_feats_t, test_feats_t"
      ],
      "metadata": {
        "id": "aQvJE454atFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* FUNCTION TO CHECK THE ACCURACY\n",
        "\n"
      ],
      "metadata": {
        "id": "JOnz4h-GbEKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checkAccuracy(train_fests_t, test_feats_t, train_y, test_y):\n",
        "  test_pred = NN(train_feats_t, train_y, test_feats_t)\n",
        "  acc = Accuracy(test_y, test_pred)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "gBusEqFQbEg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* FUNCTION TO MAKE APPLY THE TRANSFORMATION AS WELL AS GETTING THE ACCURACY\n",
        "\n"
      ],
      "metadata": {
        "id": "38uyQuI_awEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getAccuracy(train_X, train_y, test_X, test_y, featurelist, transformlist):\n",
        "  listtest = []\n",
        "  listtrain = []\n",
        "  accuracyList = []\n",
        "  # for i in range(-5,6):\n",
        "  #   transformlist.append(np.array([[i, i+1], [i-1, i]]))\n",
        "  for feature in featurelist:\n",
        "    if feature == \"boundary\":\n",
        "      listtrain.append(np.array([getBoundaryPixels(i).sum() for i in train_X]))\n",
        "      listtest.append(np.array([getBoundaryPixels(i).sum() for i in test_X]))\n",
        "    if feature == \"hole\":\n",
        "      listtrain.append(np.array([getHolePixels(i).sum() for i in train_X]))\n",
        "      listtest.append(np.array([getHolePixels(i).sum() for i in test_X]))\n",
        "    if feature == \"hull\":\n",
        "      listtrain.append(np.array([getHullPixels(i).sum() for i in train_X]))\n",
        "      listtest.append(np.array([getHullPixels(i).sum() for i in test_X]))\n",
        "    if feature == \"sum\":\n",
        "      listtrain.append(np.sum(train_X, (1,2))/(28*28))\n",
        "      listtest.append(np.sum(test_X, (1,2))/(28*28))\n",
        "  train_feats = np.vstack(tuple(j for j in listtrain)).transpose()\n",
        "  test_feats = np.vstack(tuple(j for j in listtest)).transpose()\n",
        "  # train_fests_t, test_feats_t = tranformDataSet(np.array([[0.5,-0.5],[0,2.5]]), train_feats, test_feats)\n",
        "  # accuracyList.append(checkAccuracy(train_fests_t, test_feats_t, train_y, test_y))\n",
        "  for i in transformlist:\n",
        "    train_fests_t, test_feats_t = tranformDataSet(i, train_feats, test_feats)\n",
        "    accuracyList.append(checkAccuracy(train_fests_t, test_feats_t, train_y, test_y))\n",
        "  return accuracyList"
      ],
      "metadata": {
        "id": "BE_9Gb9rbQRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* NOW LET US CHECK FOR ACCURACIES OBTAINED FOR DIFFERENT VALUES OF TRANSFORMATION MATRICES\n",
        "\n"
      ],
      "metadata": {
        "id": "7q2y_dGIbWfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "featurelist = ['hole','boundary']\n",
        "transformlist = [np.array([[0.5,-0.5],[0,2.5]]), \n",
        "                 np.array([[0.75,-0.5], [0, 2.0]]),\n",
        "                 np.array([[1,-0.5], [0, 2.0]]),\n",
        "                 np.array([[0.5,-0.5], [0, 2.0]]),\n",
        "                 np.array([[0.5,-0.75], [0, 2.5]]),\n",
        "                 np.array([[0.5,-0.5], [0, 3.0]]),\n",
        "                 np.array([[0.5,-0.5],[0,-2.5]]),\n",
        "                 np.array([[-0.5,-0.5],[0,-2.5]]),\n",
        "                 ]\n",
        "accuracyList = getAccuracy(train_X, train_y, test_X, test_y, featurelist, transformlist)\n",
        "for i in range(len(transformlist)):\n",
        "  print(f\"matrics = {transformlist[i]} accuracy = {accuracyList[i]}\")\n"
      ],
      "metadata": {
        "id": "H03tYDalbaAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* We can observe that for different values of the transformation matrix the accuracies are different. With the highest values obtained for given matrix and some of the variants that I tried. The accuracy is very sensitive with boundary values as when i tried changing them the accuracy decrease drastically. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QqYIX3QzbfMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANSWER FOR QUESTION 2**"
      ],
      "metadata": {
        "id": "VrngnAWWbnEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featurelist = [\"hull\",'sum']\n",
        "transformlist = [np.array([[0.5,-0.5],[0,2.5]]), \n",
        "                 np.array([[0.75,-0.5], [0, 2.0]]),\n",
        "                 np.array([[1,-0.5], [0, 2.0]]),\n",
        "                 np.array([[0.5,-0.5], [0, 2.0]]),\n",
        "                 np.array([[0.5,-0.75], [0, 2.5]]),\n",
        "                 np.array([[0.5,-0.5], [0, 3.0]]),\n",
        "                 np.array([[0.5,-0.5],[0,-2.5]]),\n",
        "                 np.array([[30,20],[0,-20.5]]),\n",
        "                 ]\n",
        "accuracyList = getAccuracy(train_X, train_y, test_X, test_y, featurelist, transformlist)\n",
        "for i in range(len(transformlist)):\n",
        "  print(f\"matrics = {transformlist[i]} accuracy = {accuracyList[i]}\")"
      ],
      "metadata": {
        "id": "CWuieOEQbr4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36TOA47xak20"
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "Sometimes the features of our data have vastly different scales. This will cause the learning algorithm to give more importance to certain features, reducing its performance. Data normalization is a method in which we transform the features so that they have similar scales. \n",
        "\n",
        "Three commonly used feature scaling techniques are rescaling, mean normalization and z-score normalization. Here, we will talk about the simplest one: rescaling. \n",
        "\n",
        "$$\\begin{equation}\n",
        "x' = \\frac {x -min(x)} { max(x) - min(x)}\n",
        "\\end{equation}$$\n",
        "\n",
        "\n",
        "\n",
        "For more information, see [this page](https://towardsdatascience.com/data-normalization-in-machine-learning-395fdec69d02)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni19QKDLZzeo"
      },
      "source": [
        "def rescale(data):\n",
        "  return (data - data.min())/(data.max() - data.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k83ZMtKeZrQ"
      },
      "source": [
        "We have to apply the rescaling to each feature individually. Also remember to apply the same transform we are using on the train set to the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JLOPwhvehpR"
      },
      "source": [
        "train_feats_rescaled_x = rescale(train_feats[:,0])\n",
        "train_feats_rescaled_y = rescale(train_feats[:,1])\n",
        "train_feats_rescaled = np.stack((train_feats_rescaled_x, train_feats_rescaled_y),1)\n",
        "\n",
        "test_feats_rescaled_x = rescale(test_feats[:,0])\n",
        "test_feats_rescaled_y = rescale(test_feats[:,1])\n",
        "test_feats_rescaled = np.stack((test_feats_rescaled_x, test_feats_rescaled_y),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaZVy9vxfKwX"
      },
      "source": [
        "Let us plot the rescaled features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXyatpwZevOH"
      },
      "source": [
        "fig, ax = plotData(train_feats_rescaled, train_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBgxE6zglsL"
      },
      "source": [
        "This type of rescaling makes all the features between 0 and 1. \n",
        "\n",
        "Let us calculate the accuracy obtained by this transform:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKQnsj-KgNZc"
      },
      "source": [
        "test_pred = NN(train_feats_rescaled, train_y, test_feats_rescaled)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print('Accuracy after transform = ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qYX90Gqg-jO"
      },
      "source": [
        "All 2D linear transformations can be repreented by a transformation matrix. So what is the matrix associated with the rescaling function? Actually, we cannot represent rescaling with a matrix multiplication, because it is not a linear transform. Rescaling involves shifting the origin of the data, which is not allowed under linear transformations.\n",
        "\n",
        "We can represent rescaling as a matrix multiplication followed by a vector addition. Let our first feature vector be called X and second feature vector be called Y. Suppose we want to rescale a data point [a,b]\n",
        "\n",
        "$$ \\begin{equation}\n",
        " \\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} = \n",
        " \\left\\{  \\begin{aligned} \\frac{a - min(X)}{max(X) - min(X)} \\\\ \\frac{b - min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned}&\\frac{1}{max(X)-min(X)} &0\\\\ &0 &\\frac{1}{max(Y)-min(Y)} \\end{aligned} \n",
        " \\right\\}\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\} + \n",
        " \\left\\{  \\begin{aligned} \\frac{ -min(X)}{max(X) - min(X)} \\\\ \\frac{-min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "\n",
        "You can verify this yourself if you wish, though it is not necessary. \n",
        "\n"
      ]
    }
  ]
}